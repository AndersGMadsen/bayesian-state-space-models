{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_Y8Q6VdxMTN"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KnMCukZvxul2"
      },
      "source": [
        "## Chapter 4: Kalman Filtering and Exact Solutions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JoBiUD3CFWYa"
      },
      "source": [
        "Markovian Model:\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2cIoLVRhEvxP"
      },
      "source": [
        "$$\n",
        "x_k \\sim p(x_k | x_{k-1})\n",
        "$$\n",
        "\n",
        "$$\n",
        "y_k \\sim p(y_k | x_{k})\n",
        "$$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bY7CsnLbF4nq"
      },
      "source": [
        "- Markov Property\n",
        "- Conditional independance of measurements"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0EbhLY38GDf3"
      },
      "source": [
        "Bayesian Filtering: Finding the marginal posterior\n",
        "\n",
        "$$\n",
        "p(x_k | y_{1:k})\n",
        "$$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zgud6wUVGSS_"
      },
      "source": [
        "# Kalman Filtering\n",
        "\n",
        "The closed form solution to the recursive Bayesian filtering equations"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zOeEGDCmGSVo"
      },
      "source": [
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{x}_k & =\\mathbf{A}_{k-1} \\mathbf{x}_{k-1}+\\mathbf{q}_{k-1} \\\\\n",
        "\\mathbf{y}_k & =\\mathbf{H}_k \\mathbf{x}_k+\\mathbf{r}_k\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dd5bZaBAGSZA"
      },
      "source": [
        "The dynamic and measurement models are linear Gaussian"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "enHoGJLBG69z"
      },
      "source": [
        "$$\n",
        "\\begin{aligned}\n",
        "p\\left(\\mathbf{x}_k \\mid \\mathbf{x}_{k-1}\\right) & =\\mathrm{N}\\left(\\mathbf{x}_k \\mid \\mathbf{A}_{k-1} \\mathbf{x}_{k-1}, \\mathbf{Q}_{k-1}\\right), \\\\\n",
        "p\\left(\\mathbf{y}_k \\mid \\mathbf{x}_k\\right) & =\\mathrm{N}\\left(\\mathbf{y}_k \\mid \\mathbf{H}_k \\mathbf{x}_k, \\mathbf{R}_k\\right) .\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Wdfz8T2EG7At"
      },
      "source": [
        "Evaluating the closed form results in Gaussian distributions. \n",
        "\n",
        "$$\n",
        "p\\left(\\mathbf{x}_k \\mid \\mathbf{y}_{1: k}\\right)=\\mathrm{N}\\left(\\mathbf{x}_k \\mid \\mathbf{m}_k, \\mathbf{P}_k\\right),\n",
        "$$\n",
        "\n",
        "The iterative process of estimating a new state involves a prediction step"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "th5gSK3uHhXz"
      },
      "source": [
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{m}_k^{-} & =\\mathbf{A}_{k-1} \\mathbf{m}_{k-1} \\\\\n",
        "\\mathbf{P}_k^{-} & =\\mathbf{A}_{k-1} \\mathbf{P}_{k-1} \\mathbf{A}_{k-1}^{\\top}+\\mathbf{Q}_{k-1},\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LMp8-8-rHzYg"
      },
      "source": [
        "and a update step"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dOeCvdM8Hzcu"
      },
      "source": [
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{v}_k & =\\mathbf{y}_k-\\mathbf{H}_k \\mathbf{m}_k^{-}, \\\\\n",
        "\\mathbf{S}_k & =\\mathbf{H}_k \\mathbf{P}_k^{-} \\mathbf{H}_k^{\\top}+\\mathbf{R}_k, \\\\\n",
        "\\mathbf{K}_k & =\\mathbf{P}_k^{-} \\mathbf{H}_k^{\\top} \\mathbf{S}_k^{-1}, \\\\\n",
        "\\mathbf{m}_k & =\\mathbf{m}_k^{-}+\\mathbf{K}_k \\mathbf{v}_k, \\\\\n",
        "\\mathbf{P}_k & =\\mathbf{P}_k^{-}-\\mathbf{K}_k \\mathbf{S}_k \\mathbf{K}_k^{\\top} .\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XWohomZQH6-h"
      },
      "source": [
        "## Chapter 5: Extended and Unscented Kalman filter\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{x}_k & =\\mathbf{f}\\left(\\mathbf{x}_{k-1}\\right)+\\mathbf{q}_{k-1}, \\\\\n",
        "\\mathbf{y}_k & =\\mathbf{h}\\left(\\mathbf{x}_k\\right)+\\mathbf{r}_k,\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LyFGkCf8Wy4q"
      },
      "source": [
        "Assume Gaussian Approximations\n",
        "\n",
        "$$\n",
        "p\\left(\\mathbf{x}_k \\mid \\mathbf{y}_{1: k}\\right) \\simeq \\mathrm{N}\\left(\\mathbf{x}_k \\mid \\mathbf{m}_k, \\mathbf{P}_k\\right)\n",
        "$$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QgmPs6m-Wy8B"
      },
      "source": [
        "In Extendted Kalman Filtering (EKF), we use Taylor series to approximate the non-linearities. The prediction step then becomes\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{m}_k^{-} & =\\mathbf{f}\\left(\\mathbf{m}_{k-1}\\right), \\\\\n",
        "\\mathbf{P}_k^{-} & =\\mathbf{F}_{\\mathbf{x}}\\left(\\mathbf{m}_{k-1}\\right) \\mathbf{P}_{k-1} \\mathbf{F}_{\\mathbf{x}}^{\\top}\\left(\\mathbf{m}_{k-1}\\right)+\\mathbf{Q}_{k-1},\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "and the update step becomes\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{v}_k & =\\mathbf{y}_k-\\mathbf{h}\\left(\\mathbf{m}_k^{-}\\right), \\\\\n",
        "\\mathbf{S}_k & =\\mathbf{H}_{\\mathbf{x}}\\left(\\mathbf{m}_k^{-}\\right) \\mathbf{P}_k^{-} \\mathbf{H}_{\\mathbf{x}}^{\\top}\\left(\\mathbf{m}_k^{-}\\right)+\\mathbf{R}_k, \\\\\n",
        "\\mathbf{K}_k & =\\mathbf{P}_k^{-} \\mathbf{H}_{\\mathbf{x}}^{\\top}\\left(\\mathbf{m}_k^{-}\\right) \\mathbf{S}_k^{-1}, \\\\\n",
        "\\mathbf{m}_k & =\\mathbf{m}_k^{-}+\\mathbf{K}_k \\mathbf{v}_k, \\\\\n",
        "\\mathbf{P}_k & =\\mathbf{P}_k^{-}-\\mathbf{K}_k \\mathbf{S}_k \\mathbf{K}_k^{\\top} .\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "\n",
        "In essense, this means using the Jacobian of the transition function and the measurement model matrix, respectively. Other versions exist that take non-additive and additive noise into account. Could be relevant!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zJvL3yyYWy_b"
      },
      "source": [
        "Unscented Kalman Filtering (UKF) uses the unscented transformation and is used for approximating distributations of the same form as EKF. Prediction:\n",
        "\n",
        "- Prediction:\n",
        "\n",
        "\n",
        "1. Form the sigma points:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathcal{X}_{k-1}^{(0)} & =\\mathbf{m}_{k-1}, \\\\\n",
        "\\mathcal{X}_{k-1}^{(i)} & =\\mathbf{m}_{k-1}+\\sqrt{n+\\lambda}\\left[\\sqrt{\\mathbf{P}_{k-1}}\\right]_i, \\\\\n",
        "\\mathcal{X}_{k-1}^{(i+n)} & =\\mathbf{m}_{k-1}-\\sqrt{n+\\lambda}\\left[\\sqrt{\\mathbf{P}_{k-1}}\\right]_i, \\quad i=1, \\ldots, n,\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "2. Propagate the sigma points through the dynamic model:\n",
        "$$\n",
        "\\hat{\\mathcal{X}}_k^{(i)}=\\mathbf{f}\\left(\\mathcal{X}_{k-1}^{(i)}\\right), \\quad i=0, \\ldots, 2 n .\n",
        "$$\n",
        "\n",
        "\n",
        "3. Compute the predicted mean $\\mathbf{m}_k^{-}$and the predicted covariance $\\mathbf{P}_k^{-}$:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "& \\mathbf{m}_k^{-}=\\sum_{i=0}^{2 n} W_i^{(\\mathrm{m})} \\hat{\\mathcal{X}}_k^{(i)}, \\\\\n",
        "& \\mathbf{P}_k^{-}=\\sum_{i=0}^{2 n} W_i^{(\\mathrm{c})}\\left(\\hat{\\mathcal{X}}_k^{(i)}-\\mathbf{m}_k^{-}\\right)\\left(\\hat{\\mathcal{X}}_k^{(i)}-\\mathbf{m}_k^{-}\\right)^{\\top}+\\mathbf{Q}_{k-1},\n",
        "\\end{aligned}\n",
        "$$\n",
        "where the weights $W_i^{(\\mathrm{m})}$ and $W_i^{(\\mathrm{c})}$ were defined in Equation (5.77)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iHZ5KCO9WzCR"
      },
      "source": [
        "Update: \n",
        "\n",
        "1. Form Sigma Points\n",
        "2.Propagate sigma points through the measurement model\n",
        "3. Compute predicted mean $\\mu_k$, covariamce of measurement $S_k$, and cross-covariance, $C_k$\n",
        "4. Compute filter gain $K_k = C_k S_k^{-1}$, filtered state mean $m_k$, and covariance $P_k$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "81ggY8WAxaS1"
      },
      "source": [
        "## Chapter 8: Rauch-Tung-Striebel (RTS) smoother\n",
        "\n",
        "$$\n",
        "p\\left(\\mathbf{x}_k \\mid \\mathbf{y}_{1:T}\\right)=\\mathrm{N}\\left(\\mathbf{x}_k \\mid \\mathbf{m}_k^s, \\mathbf{P}_k^s\\right),\n",
        "$$\n",
        "\n",
        "Same predictions as Kalman\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{m}_{k+1}^{-} & =\\mathbf{A}_{k} \\mathbf{m}_{k} \\\\\n",
        "\\mathbf{P}_{k+1}^{-} & =\\mathbf{A}_{k} \\mathbf{P}_{k} \\mathbf{A}_{k}^{\\top}+\\mathbf{Q}_{k},\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Backward recursion\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MO_0eFaGzJLJ"
      },
      "source": [
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{G}_k & =\\mathbf{P}_k \\mathbf{A}_k^{\\top} [\\mathbf{P}_{k+1}]^{-1}, \\\\\n",
        "\\mathbf{m}_k^s & =\\mathbf{m}_k+\\mathbf{G}_k [\\mathbf{m}_{k+1}^s - \\mathbf{m}_{k+1}^-], \\\\\n",
        "\\mathbf{P}_k^s & =\\mathbf{P}_k+\\mathbf{\n",
        "G}_k [\\mathbf{P}_{k+1}^s -\\mathbf{P}_{k+1}^-] \\mathbf{G}_k^T .\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Dnqk_8s10FXC"
      },
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "l7BP_u3v1A27"
      },
      "source": [
        "## Particle Filtering\n",
        "\n",
        "The main issue in bayesian inference is computing the posterior, or the expectation of the prosterior, i.e., \n",
        "\n",
        "$$\n",
        "\\mathbb{E}\\left[\\mathrm{g}\\left(\\mathbf{x}\\right) \\mid {y}_{1:T}\\right]=\\int\\mathrm{g}\\left(\\mathbf{x}\\right) p(x \\mid y_{1:T}) dx,\n",
        "$$\n",
        "\n",
        "where $g$ is an arbitrary (possibly nonlinear) function. Monte Carlo methods provides a numerical method for approximating the expectation. In a perfect setting, we simply sample from the posterior and compute the average, i.e.,\n",
        "\n",
        "$$\n",
        "\\mathbb{E}\\left[\\mathrm{g}\\left(\\mathbf{x}\\right)  \\mid {y}_{1:T}\\right]=\\frac{1}{N} \\sum \\mathrm{g}\\left(\\mathbf{x}^{(i)}\\right) .\n",
        "$$\n",
        "\n",
        "However, this method can be very cost efficient - especially if $g$ is nonlinear - and it might even be impossible to sample from the posterior. Hence, we introduce importance sampling.\n",
        "\n",
        "Importance sampling build on the assumption that we can approximate the posterior using an importance distribution $\\pi$ from which we can easily draw samples. We draw samples,\n",
        "\n",
        "$$\n",
        "\\pi(x \\mid y_{1:T}), \\quad i = 1,...,N,\n",
        "$$\n",
        "\n",
        "and form the approximation \n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathrm{E}\\left[\\mathrm{g}(\\mathbf{x}) \\mid \\mathbf{y}_{1: T}\\right] & \\approx \\frac{1}{N} \\sum_{i=1}^N \\frac{p\\left(\\mathbf{x}^{(i)} \\mid \\mathbf{y}_{1: T}\\right)}{\\pi\\left(\\mathbf{x}^{(i)} \\mid \\mathbf{y}_{1: T}\\right)} \\mathrm{g}\\left(\\mathbf{x}^{(i)}\\right) \\\\\n",
        "& =\\sum_{i=1}^N \\tilde{w}^{(i)} \\mathrm{g}\\left(\\mathbf{x}^{(i)}\\right),\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "where $\\pi$ must have a support greater or equal to the posterior. In this way, weights are used to attribute $\\textit{importance}$ to samples approximating the posterior probability density. \n",
        "\n",
        "\n",
        "Importance sampling approximations can be used sequentially to generate filtering distributions of generic state space models. At each step, the expected value can be found using importance sampling. A problem in the sequential importance sampling (SIS) arises when nearly all weights are equal to zero. This is called the $\\textit{degeneracy problem}$. Several methods exist for resampling particles - these are called sequential importance resampling (SIR). Briefly, the resampling procedure can be described as follows:\n",
        "\n",
        "1. Interpret each weight $w_k^{(i)}$ as the probability of obtaining the sample index $i$.\n",
        "2. Draw new samples from this discrete distribution.\n",
        "3. Set all weights to $1/N$.\n",
        "\n",
        "This procedure is normally what is referred to as the particle filter.\n",
        "\n",
        "\n",
        "With the Rao-Blackwellized particle filter, we assume that we can sometimes evaulate some of the filter equations analitically and the rest with Monte Carlo. This is an improvement as it reduces variance. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
